{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# nltk imports\n",
    "import nltk"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%% imports\n"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "basePath = os.path.abspath('') + \"\\\\sentiment-analysis-nlp\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "['So there is no way for me to plug it in here in the US unless I go by a converter.\\t0\\n',\n 'Good case, Excellent value.\\t1\\n',\n 'Great for the jawbone.\\t1\\n',\n 'Tied to charger for conversations lasting more than 45 minutes.MAJOR PROBLEMS!!\\t0\\n',\n 'The mic is great.\\t1\\n',\n 'I have to jiggle the plug to get it to line up right to get decent volume.\\t0\\n',\n 'If you have several dozen or several hundred contacts, then imagine the fun of sending each of them one by one.\\t0\\n',\n 'If you are Razr owner...you must have this!\\t1\\n',\n 'Needless to say, I wasted my money.\\t0\\n',\n 'What a waste of money and time!.\\t0\\n']"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = basePath + \"NLP\\\\sentiment_labelled_sentences\\\\full_set.txt\"\n",
    "with open(file) as f:\n",
    "    content = f.readlines()\n",
    "\n",
    "content[0:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Removing white spaces\n",
    "content = [x.strip() for x in content]\n",
    "\n",
    "# Separating sentences from labels\n",
    "sentences = [x.split(\"\\t\")[0] for x in content]\n",
    "labels = [x.split(\"\\t\")[1] for x in content]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "['So there is no way for me to plug it in here in the US unless I go by a converter.',\n 'Good case, Excellent value.',\n 'Great for the jawbone.',\n 'Tied to charger for conversations lasting more than 45 minutes.MAJOR PROBLEMS!!',\n 'The mic is great.',\n 'I have to jiggle the plug to get it to line up right to get decent volume.',\n 'If you have several dozen or several hundred contacts, then imagine the fun of sending each of them one by one.',\n 'If you are Razr owner...you must have this!',\n 'Needless to say, I wasted my money.',\n 'What a waste of money and time!.']"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "['0', '1', '1', '0', '1', '0', '0', '1', '0', '0']"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "array([-1,  1,  1, ..., -1, -1, -1], dtype=int8)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Transforming the labels to go from -1 to 1, instead of 0 to 1\n",
    "-1 represents negative, +1 represents positive\n",
    "'''\n",
    "\n",
    "y = np.array(labels, dtype='int8')\n",
    "y = 2*y - 1\n",
    "y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "['so there is no way for me plug in here in us unless go by converter',\n 'good case excellent value',\n 'great for jawbone',\n 'tied charger for conversations lasting more than minutes major problems',\n 'mic is great',\n 'have jiggle plug get line up right get decent volume',\n 'if you have several dozen or several hundred contacts then imagine fun sending each them one by one',\n 'if you are razr owner you must have this',\n 'needless say wasted my money',\n 'what waste money and time']"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing extras - stopwords, digits, punctuation\n",
    "\n",
    "def remove_elements(x, removal_list):\n",
    "    for z in removal_list:\n",
    "        x = x.replace(z, ' ')\n",
    "    return x\n",
    "\n",
    "# Removing digits\n",
    "digit_list = [str(x) for x in range(10)]\n",
    "digits_removed = [remove_elements(x, digit_list) for x in sentences]\n",
    "\n",
    "# Removing punctuations\n",
    "punctuations_removed = [remove_elements(x, string.punctuation) for x in digits_removed]\n",
    "\n",
    "# Converting to lower case and removing whitespaces\n",
    "sentences = [x.lower() for x in punctuations_removed]\n",
    "sentences = [x.strip() for x in sentences]\n",
    "\n",
    "# Removing stopwords\n",
    "def remove_stopwords(stopword, text):\n",
    "    new_text = ' '.join([word for word in text.split() if word not in stopword])\n",
    "    return new_text\n",
    "\n",
    "# Defining our own set of stopwords\n",
    "stop_set = ['the', 'a', 'an', 'i', 'he', 'she', 'they', 'to', 'of', 'it', 'from']\n",
    "preprocessed = [remove_stopwords(stop_set, x) for x in sentences]\n",
    "preprocessed[0:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def porter_stemmer(words):\n",
    "    porter = nltk.PorterStemmer()\n",
    "    stemmed = [porter.stem(word) for word in words]\n",
    "    return stemmed\n",
    "\n",
    "stemmed_sentences = [porter_stemmer(words.split()) for words in preprocessed]\n",
    "stemmed_sentences = [\" \".join(i) for i in stemmed_sentences]\n",
    "stemmed_sentences[0:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "['so there is no way for me plug in here in us unless go by convert',\n 'good case excel valu',\n 'great for jawbon',\n 'tie charger for convers last more than minut major problem',\n 'mic is great',\n 'have jiggl plug get line up right get decent volum',\n 'if you have sever dozen or sever hundr contact then imagin fun send each them one by one',\n 'if you are razr owner you must have thi',\n 'needless say wast my money',\n 'what wast money and time']"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TD/IDF\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "vectorizer = CountVectorizer(analyzer=\"word\",\n",
    "                             preprocessor=None,\n",
    "                             stop_words='english',\n",
    "                             max_features=6000,\n",
    "                             ngram_range=(1,5))\n",
    "\n",
    "date_features = vectorizer.fit_transform(preprocessed)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "data_features_tfidf = tfidf_transformer.fit_transform(date_features)\n",
    "data_matrix = data_features_tfidf.toarray()\n",
    "data_matrix"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Creating Training and Test Sets\n",
    "\n",
    "np.random.seed(0)\n",
    "test_index = np.append(np.random.choice((np.where(y==-1))[0], 250, replace=False),\n",
    "                       np.random.choice((np.where(y==1))[0], 250, replace=False))\n",
    "train_index = list(set(range(len(labels))) - set(test_index))\n",
    "\n",
    "train_data = data_matrix[train_index,]\n",
    "train_labels = y[train_index]\n",
    "\n",
    "test_data = data_matrix[test_index,]\n",
    "test_labels = y[test_index]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# Working with TextBlob\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "## Creating polarity and subjectivity functions\n",
    "\n",
    "polarity_function = lambda x: TextBlob(x).sentiment.polarity\n",
    "subjectivity_function = lambda x: TextBlob(x).sentiment.subjectivity\n",
    "polarity_list = [polarity_function(x) for x in preprocessed]\n",
    "subjectivity_list = [subjectivity_function(x) for x in preprocessed]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "[0.0,\n 0.85,\n 0.8,\n 0.1875,\n 0.8,\n 0.22619047619047616,\n 0.09999999999999999,\n 0.0,\n -0.35,\n -0.2]"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polarity_list[0:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "[0.0,\n 0.8,\n 0.75,\n 0.3333333333333333,\n 0.75,\n 0.6011904761904762,\n 0.06666666666666667,\n 0.0,\n 0.5,\n 0.0]"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subjectivity_list[0:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "## Fitting classifier on training data\n",
    "classifier = SGDClassifier(loss=\"log\", penalty=\"none\")\n",
    "classifier.fit(train_data, train_labels)\n",
    "\n",
    "## Pull out the parameters (w,b) of the logistic regression model\n",
    "w = classifier.coef_[0, :]\n",
    "b = classifier.intercept_\n",
    "\n",
    "## Get predictions on training and test data\n",
    "predictions_train = classifier.predict(train_data)\n",
    "predictions_test = classifier.predict(test_data)\n",
    "\n",
    "## Computing errors\n",
    "error_training = np.sum((predictions_train > 0.0 ) != (train_labels > 0.0))\n",
    "error_testing = np.sum((predictions_test > 0.0) != (test_labels > 0.0))\n",
    "\n",
    "training_error = float(error_training) / len(train_labels)\n",
    "testing_error = float(error_testing) / len(test_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "0.0116"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_error"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "0.184"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_error"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# Finding words with strong influence\n",
    "\n",
    "## Converting vocabulary into a list\n",
    "vocab = np.array([z[0] for z in sorted(vectorizer.vocabulary_.items(), key=lambda x:x[1])])\n",
    "\n",
    "## Getting indices by sorting w\n",
    "indices = np.argsort(w)\n",
    "\n",
    "## Words with large negative value\n",
    "negative_indices = indices[0:50]\n",
    "negative_words = [str(x) for x in list(vocab[negative_indices])]\n",
    "\n",
    "positive_indices = indices[-49:-1]\n",
    "positive_words = [str(x) for x in list(vocab[positive_indices])]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sucks', 'worst', 'poor', 'bad', 'disappointing', 'bland', 'disappointment', 'horrible', 'failed', 'avoid', 'cheap', 'stupid', 'unfortunately', 'doesn', 'sucked', 'rude', 'average', 'fly', 'slow', 'probably', 'piece', 'tasteless', 'awful', 'mistake', 'return', 'directing', 'dirty', 'dropped', 'blah', 'junk', 'mediocre', 'waste', 'waste time', 'appealing', 'selection food', 'flat', 'improvement', 'ok', 'torture', 'hour', 'wasted', 'eating', 'att', 'engaging', 'poorly', 'happened', 'joke', 'crap', 'remorse', 'didn']\n"
     ]
    }
   ],
   "source": [
    "print(negative_words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['exactly', 'forget', 'vegas buffet', 'crisp', 'haven', 'inside', 'entertaining', 'score', 'fun', 'art', 'fast', 'friendly', 'works great', 'highly recommend', 'audio', 'favorite', 'fall', 'hand', 'shows', 'definately', 'pleased', 'played', 'plus', 'prices', 'comfortable', 'fabulous', 'wonderful', 'bacon', 'soundtrack', 'fantastic', 'incredible', 'cool', 'delicious', 'won disappointed', 'best', 'awesome', 'assure', 'beautiful', 'excellent', 'liked', 'perfect', 'works', 'amazing', 'loved', 'enjoyed', 'interesting', 'nice', 'great']\n"
     ]
    }
   ],
   "source": [
    "print(positive_words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "0.174"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb_classifier = MultinomialNB().fit(train_data, train_labels)\n",
    "\n",
    "nb_predictions_test = nb_classifier.predict(test_data)\n",
    "nb_error_testing = np.sum((nb_predictions_test > 0.0) != (test_labels > 0.0))\n",
    "nb_error_testing = float(nb_error_testing) / len(test_labels)\n",
    "nb_error_testing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "['positive', 'negative', 'negative', 'positive']"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Testing the model trained of Naive Baiyes\n",
    "\n",
    "list_reviews = [\n",
    "    [\"It's a sad movie but very good\"],\n",
    "    [\"Waste of my time\"],\n",
    "    [\"It is not what like\"],\n",
    "    [\"It is not what I m looking for\"]\n",
    "]\n",
    "\n",
    "polarity_index = {-1 : \"negative\",  0 : \"neutral\", 1 : \"positive\"}\n",
    "sentiment_test_of_reviews = []\n",
    "for x in list_reviews:\n",
    "    sentiment = nb_classifier.predict(vectorizer.transform(x))[0]\n",
    "    sentiment_test_of_reviews.append(polarity_index[sentiment])\n",
    "\n",
    "sentiment_test_of_reviews"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}