{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# NLP Sentiment Analysis Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# nltk imports\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "basePath = os.path.abspath('') + \"\\\\sentiment-analysis-nlp\\\\\"\n",
    "basePath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "file = basePath + \"NLP\\\\sentiment_labelled_sentences\\\\full_set.txt\"\n",
    "with open(file) as f:\n",
    "    content = f.readlines()\n",
    "\n",
    "content[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Text preprocessing** is traditionally an important step for natural language processing (NLP) tasks. It transforms text into a more digestible form so that machine learning algorithms can perform better.\n",
    "\n",
    "The various Text Preprocesing steps are -\n",
    "* Tokenization - Splitting the sentence into words.\n",
    "* Lower casing\n",
    "* Stopwords removal - Stop words are very commonly used words (a, an, the, etc.) in the documents. These words do not really signify any importance as they do not help in distinguishing two documents.\n",
    "* Stemming: It is a process of transforming a word to its root form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1. Extracting sentences and creating labels from dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this step, we are cleaning our dataset. This is the first step of preprocessing. Sentences are a collection of words that end with a punctuation. We are extracting individual sentences from a review and creating labels for them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Removing white spaces\n",
    "content = [x.strip() for x in content]\n",
    "\n",
    "# Separating sentences from labels\n",
    "sentences = [x.split(\"\\t\")[0] for x in content]\n",
    "labels = [x.split(\"\\t\")[1] for x in content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sentences[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "labels[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Transforming the labels to go from -1 to 1, instead of 0 to 1\n",
    "-1 represents negative, +1 represents positive\n",
    "'''\n",
    "\n",
    "y = np.array(labels, dtype='int8')\n",
    "y = 2*y - 1\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2. Removing extras - stopwords, digits, punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Here, we are removing all the non-essential parts in our sentences. Digits and punctuation do not contribute to the sentiment of the sentence and are thus being removed.\n",
    "We are also removing stopwords from the sentence. For stopwords, we have defined our own set of stopwords rather than taking from a corpus. This is done to prevent the reduction fo reviews to mere words that are useless to the application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def remove_elements(x, removal_list):\n",
    "    for z in removal_list:\n",
    "        x = x.replace(z, ' ')\n",
    "    return x\n",
    "\n",
    "# Removing digits\n",
    "digit_list = [str(x) for x in range(10)]\n",
    "digits_removed = [remove_elements(x, digit_list) for x in sentences]\n",
    "\n",
    "# Removing punctuations\n",
    "punctuations_removed = [remove_elements(x, string.punctuation) for x in digits_removed]\n",
    "\n",
    "# Converting to lower case and removing whitespaces\n",
    "sentences = [x.lower() for x in punctuations_removed]\n",
    "sentences = [x.strip() for x in sentences]\n",
    "\n",
    "# Removing stopwords\n",
    "def remove_stopwords(stopword, text):\n",
    "    new_text = ' '.join([word for word in text.split() if word not in stopword])\n",
    "    return new_text\n",
    "\n",
    "# Defining our own set of stopwords\n",
    "stop_set = ['the', 'a', 'an', 'i', 'he', 'she', 'they', 'to', 'of', 'it', 'from']\n",
    "preprocessed = [remove_stopwords(stop_set, x) for x in sentences]\n",
    "preprocessed[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Stemming (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def porter_stemmer(words):\n",
    "    porter = nltk.PorterStemmer()\n",
    "    stemmed = [porter.stem(word) for word in words]\n",
    "    return stemmed\n",
    "\n",
    "stemmed_sentences = [porter_stemmer(words.split()) for words in preprocessed]\n",
    "stemmed_sentences = [\" \".join(i) for i in stemmed_sentences]\n",
    "stemmed_sentences[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. TD/IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This is a measure of the relative importance of a word within a document, in the context of multiple documents. In our case here, multiple reviews.\n",
    "We start with the TD part — this is simply a normalized frequency of the word in the document:\n",
    "`(word count in document) / (total words in document)`\n",
    "\n",
    "The IDF is a weighting of the uniquess of the word across all of the documents. Here is the complete formula of TD/IDF:\n",
    "`td_idf(t,d) = wc(t,d)/wc(d) / dc(t)/dc()`\n",
    "where:\n",
    "* `wc(t,d)` = # of occurrences of term t in doc d\n",
    "* `wc(d)` = # of words in doc d\n",
    "* `dc(t)` = # of docs that contain at least 1 occurrence of term t\n",
    "* `dc()` = # of docs in collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "vectorizer = CountVectorizer(analyzer=\"word\",\n",
    "                             preprocessor=None,\n",
    "                             stop_words='english',\n",
    "                             max_features=6000,\n",
    "                             ngram_range=(1,5))\n",
    "\n",
    "date_features = vectorizer.fit_transform(preprocessed)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "data_features_tfidf = tfidf_transformer.fit_transform(date_features)\n",
    "data_matrix = data_features_tfidf.toarray()\n",
    "data_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3. Creating Training and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "test_index = np.append(np.random.choice((np.where(y==-1))[0], 250, replace=False),\n",
    "                       np.random.choice((np.where(y==1))[0], 250, replace=False))\n",
    "train_index = list(set(range(len(labels))) - set(test_index))\n",
    "\n",
    "train_data = data_matrix[train_index,]\n",
    "train_labels = y[train_index]\n",
    "\n",
    "test_data = data_matrix[test_index,]\n",
    "test_labels = y[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4. Finding Polarity and Subjectivity (TextBlob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**TextBlob**: Linguistic researchers have labeled the sentiment of words based on their domain expertise. Sentiment of words can vary based on where it is in a sentence.\n",
    "The TextBlob module allows us to take advantage of these labels. TextBlob finds all the words and phrases that it can assign polarity and subjectivity to, and average all of them together\n",
    "\n",
    "**Sentiment Labels**: Each word in a corpus is labeled in terms of polarity and subjectivity (there are more labels as well, but we’re going to ignore them for now). A corpus’ sentiment is the average of these.\n",
    "* **Polarity**: How positive or negative a word is. -1 is very negative. +1 is very positive.\n",
    "* **Subjectivity**: How subjective, or opinionated a word is. 0 is fact. +1 is very much an opinion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "## Creating polarity and subjectivity functions\n",
    "\n",
    "polarity_function = lambda x: TextBlob(x).sentiment.polarity\n",
    "subjectivity_function = lambda x: TextBlob(x).sentiment.subjectivity\n",
    "polarity_list = [polarity_function(x) for x in preprocessed]\n",
    "subjectivity_list = [subjectivity_function(x) for x in preprocessed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "polarity_list[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "subjectivity_list[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 5. Logistic Regression\n",
    "\n",
    "Logistic regression is used to describe data and to explain the relationship between one dependent binary variable. Logistic Regression is used when the dependent variable(target) is categorical.\n",
    "\n",
    "**Types of Logistic Regression:**\n",
    "\n",
    "* **Binary Logistic Regression**: The categorical response has only two 2 possible outcomes. eg. True or false\n",
    "* **Multinomial Logistic Regression**: Three or more categories without ordering. Example: Predicting which food is preferred more (Veg, Non-Veg, Vegan)\n",
    "* **Ordinal Logistic Regression**: Three or more categories with ordering. Example: Movie rating from 1 to 5.\n",
    "\n",
    "**Decision Boundary**\n",
    "\n",
    "To predict which class a data belongs, a threshold can be set. Based upon this threshold, the obtained estimated probability is classified into classes.\n",
    "\n",
    "**SGDClassifier**\n",
    "\n",
    "Incrementally trained logistic regression. This implementation works with data represented as dense or sparse arrays of floating point values for the features. The model it fits can be controlled with the loss parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "## Fitting classifier on training data\n",
    "classifier = SGDClassifier(loss=\"log\", penalty=\"none\")\n",
    "classifier.fit(train_data, train_labels)\n",
    "\n",
    "## Pull out the parameters (w,b) of the logistic regression model\n",
    "w = classifier.coef_[0, :]\n",
    "b = classifier.intercept_\n",
    "\n",
    "## Get predictions on training and test data\n",
    "predictions_train = classifier.predict(train_data)\n",
    "predictions_test = classifier.predict(test_data)\n",
    "\n",
    "## Computing errors\n",
    "error_training = np.sum((predictions_train > 0.0 ) != (train_labels > 0.0))\n",
    "error_testing = np.sum((predictions_test > 0.0) != (test_labels > 0.0))\n",
    "\n",
    "training_error = float(error_training) / len(train_labels)\n",
    "testing_error = float(error_testing) / len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "training_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "testing_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 6. Finding words with strong influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## Converting vocabulary into a list\n",
    "vocab = np.array([z[0] for z in sorted(vectorizer.vocabulary_.items(), key=lambda x:x[1])])\n",
    "\n",
    "## Getting indices by sorting w\n",
    "indices = np.argsort(w)\n",
    "\n",
    "## Words with large negative value\n",
    "negative_indices = indices[0:50]\n",
    "negative_words = [str(x) for x in list(vocab[negative_indices])]\n",
    "\n",
    "positive_indices = indices[-49:-1]\n",
    "positive_words = [str(x) for x in list(vocab[positive_indices])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(negative_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(positive_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Making a function to test on custom reviews**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "list_reviews = [\n",
    "    [\"It's a sad movie but very good\"],\n",
    "    [\"Waste of my time\"],\n",
    "    [\"It is not what like\"],\n",
    "    [\"It is not what I m looking for\"]\n",
    "]\n",
    "\n",
    "polarity_index = {-1 : \"negative\",  0 : \"neutral\", 1 : \"positive\"}\n",
    "\n",
    "def test_on_samples(my_classifier, list_of_reviews = list_reviews):\n",
    "    sentiment_test_of_reviews = []\n",
    "    for x in list_of_reviews:\n",
    "        sentiment = my_classifier.predict(vectorizer.transform(x))[0]\n",
    "        sentiment_test_of_reviews.append(polarity_index[sentiment])\n",
    "    return sentiment_test_of_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 7. Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "It is a classification technique based on Bayes’ Theorem with an assumption of independence among predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature.\n",
    "\n",
    "*Naive Bayes classifier for multinomial models*\n",
    "\n",
    "* The multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb_classifier = MultinomialNB().fit(train_data, train_labels)\n",
    "\n",
    "nb_predictions_test = nb_classifier.predict(test_data)\n",
    "nb_error_testing = np.sum((nb_predictions_test > 0.0) != (test_labels > 0.0))\n",
    "nb_error_testing = float(nb_error_testing) / len(test_labels)\n",
    "nb_error_testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## Testing the model trained of Naive Bayes\n",
    "\n",
    "test_sentiments = test_on_samples(nb_classifier)\n",
    "test_sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 8. SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "__SVM__<br>\n",
    "A _support vector machine_ (SVM) is a supervised machine learning model that uses classification algorithms for two-group classification problems. After giving an SVM model sets of labeled training data for each category, they’re able to categorize new text.\n",
    "\n",
    "__Overview__\n",
    "A support vector machine takes these data points and outputs the hyperplane (which in two dimensions it’s simply a line) that best separates the tags. This line is the decision boundary: It will classify the elements on either side into two separate categories.  \n",
    "__Kernel Trick__\n",
    "Hyper planes take the form of a line only in the case of a 2D distribution. However th boudary ine formed may exist in different planes and dimesnions. Calculating these decision boundaries become very computation intensive as three number of dimensions increase. _Kernel Trick_ allows us to operate in the original feature space without computing the coordinates of the data in a higher dimensional space.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "svm_classifier = SGDClassifier(loss=\"hinge\", penalty=\"l2\")\n",
    "svm_classifier.fit(train_data, train_labels)\n",
    "\n",
    "svm_predict_test = svm_classifier.predict(test_data)\n",
    "svm_error_test = np.sum((svm_predict_test > 0.0) != (test_labels > 0.0))\n",
    "svm_error_test = float(svm_error_test) / len(test_labels)\n",
    "\n",
    "svm_error_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## Testing the model trained of SVM\n",
    "\n",
    "test_sentiments = test_on_samples(svm_classifier, [[\"I would not recommend this movie\"]])\n",
    "test_sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 9. LSTM Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## LSTM\n",
    "LSTM stands for Long Short Term Memory networks. They are a modification of RNN: Recurrant Neural Networks.\n",
    "\n",
    "### Preprocess\n",
    "Here, we use numpy to convert the input text into a vector of specific length and pad empty spaces to make it fit a certain dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, SpatialDropout1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "max_review_length = 200\n",
    "\n",
    "tokenizer = Tokenizer(num_words=10000,\n",
    "                      filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~',\n",
    "                      lower=True)\n",
    "tokenizer.fit_on_texts(preprocessed)\n",
    "\n",
    "### Truncate and pad input sequences\n",
    "\n",
    "X = tokenizer.texts_to_sequences(preprocessed)\n",
    "X = sequence.pad_sequences(X, maxlen=max_review_length)\n",
    "X.shape # Shape of data tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "Y = pd.get_dummies(y).values\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## Generating testing and training data\n",
    "\n",
    "np.random.seed(0)\n",
    "test_indices = np.append(np.random.choice((np.where(y == -1))[0], 250, replace=False),\n",
    "                         np.random.choice((np.where(y == 1))[0], 250, replace=False))\n",
    "test_data = X[test_indices,]\n",
    "test_labels = Y[test_indices]\n",
    "\n",
    "training_indices = list(set(range(len(labels))) - set(test_indices))\n",
    "training_data = X[training_indices,]\n",
    "training_labels = Y[training_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## Creating networks\n",
    "\n",
    "EMBEDDING_DIM = 200\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(250, dropout=0.2, return_sequences=True))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Neural Network\n",
    "As observed above, we convert the text into a vector of length 200 and use it as an input for the neural network. Here, we also define the neural network architecture.\n",
    "The model is of Sequential type and uses 2 LSTM layers with finally a dense layer with softmax activation.\n",
    "\n",
    "## Layers\n",
    "- *Embedding*: Turns positive integers (indexes) into dense vectors of fixed size.\n",
    "- *Spatial Dropout*: Help improve independence between feature maps.\n",
    "- *LSTM*: Implements an LSTM layer\n",
    "- *Dense*: Acts as a classification layer with the given activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.fit(training_data, training_labels,\n",
    "          epochs=2,\n",
    "          batch_size=40,\n",
    "          validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(test_data, test_labels, verbose=2, batch_size=40)\n",
    "print(f\"loss: {loss}\")\n",
    "print(f\"Validation accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "outcome_label = ['Negative', 'Positive']\n",
    "def decide_review(review):\n",
    "    seq = tokenizer.texts_to_sequences([review])\n",
    "    padded = sequence.pad_sequences(seq, maxlen=max_review_length)\n",
    "    predict = model.predict(padded)\n",
    "    print(\"Probability Distribution : \", predict)\n",
    "    print(outcome_label[np.argmax(predict)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "decide_review(\"It is not what I am looking for\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "decide_review(\"This isn't what I am looking for\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "decide_review(\"This is perfect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "decide_review(\"I want this now\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model Output\n",
    "As observable, the model output is a tuple of 2 numbers which was the output dimension of the last, Dense layer. The Softmax layer converted two real positive/negative numbers into the probability of their occurence and hence the 2 probabilities. The training of the model determines the tipping point for the classification."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}